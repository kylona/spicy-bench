\section{Implementation}
The data race detection technique described in this paper has been implemented for Habanero Java. It uses the verification runtime specifically designed to test HJ programs \cite{anderson2014jpf}. This runtime makes use of JPF to schedule and run the programs. The computation graphs are stored in a directed acyclic graph. The JGraphT library provides an implementation of directed acylic graph \cite{jgrapht}. This library has been used to store the computation graphs. The computation graphs are exported in the dot file format for convenience and as a way to understand the structure of the program. The implementation is written in Java. It consists of 5 classes with 1600 lines of code.

JPF is modified by removing its default scheduling factory that inserts choices on all thread actions and accesses to shared variables. Instead, a new scheduling factory based on Algorithm 2 is employed for scheduling.

JPF's VM listeners have been used to keep track of various program events. The methods \texttt{objectCreated} and \texttt{objectReleased} are used to create nodes in the computation graph. The \texttt{objectCreated} method is used to track the creation of new \textbf{async} tasks. The \textsc{Post} rule is used to add nodes to the computation graph when the \texttt{objectCreated} method returns a task object. Similarly, the \texttt{objectReleased} method is used to track when \textbf{finish} blocks complete execution. The \textsc{Await-next} rule is used to create a node in the graph where the tasks belonging to the \textbf{finish} block join.

The \texttt{executeInstruction} method is used to track memory locations that are accessed by various tasks. The current node of the task calling \texttt{executeInstruction} method is updated with the location accessed by the task during the execution of that instruction. \algoref{algo:drd} is used to analyze the computation graphs of the program. When an array is accessed by the program, every element of the array is treated as a single variable for data race detection.

\section{Results}
The results for this technique have been compared to JPF's precise race detector and gradual permission regions based race detector on benchmarks that cover a wide range of functionality in HJ. The results show a significant improvement in the time required for verification. These two approaches were specifically chosen for comparison since the results generated by these approaches are sound for a given input just like the technique discussed in this paper.

The precise race detector explores all potential executions in a systematic way. Each execution is a sequence of transitions. Each transition takes the system from one state to another. Each transition consists of a sequence of bytecode instructions. JPF groups bytecode instructions such that an instruction that manipulates a shared variable is the first one of a transition. In every state that JPF visits, the precise race detector checks all actions that can be performed next. If this collection of actions contains at least two conflicting accesses of a shared variable, then a race on the shared variable is reported.

Gradual Permission regions use program annotations to reduce the state space of the program. Whenever a shared variable is accessed by two or more tasks in the program, the accesses have to be annotated to inform the data race detector to create different schedules for these accesses. Since the method requires manual annotation of the programs, it is prone to human errors. If the program is annotated incorrectly, the results of data race detection analysis do not have any significance.

\begin{landscape}
\begin{table*}
\centering
\caption{Benchmarks of HJ programs: Computation graphs vs Permission Regions vs. PreciseRaceDetector.}
\label{tab:results}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
        &      &       & 
        \multicolumn{3}{c|}{Computation graphs} & 
		 \multicolumn{3}{c|}{Permission Regions} &
		\multicolumn{3}{c|}{Precise Race Detector} \\ \hline
		
Test ID & SLOC & Tasks 
& States  & Time  & Error Note 
& States  & Time  & Error Note 
& States  & Time  & Error Note     \\ \hline

Primitive Array & 29 & 3 
%& 5 & 00:00 & No Race
& 5 & 00:00 & No Race
& 5 & 00:00 & No Race 
& 11,852 & 00:00 & No Race \\
No Race &  & 
& & &
& & &
& & & \\ \hline

Primitive Array & 39 & 3 
%& 5 & 00:00 & Race
& 5 & 00:00 & Race
& 5 & 00:00 & Race
& 220 & 00:00 & Race \\ 
Race &  & 
& & &
& & &
& & & \\ \hline

Two Dim Arrays & 30 & 11 
%& 15 & 00:01 & No Race
& 15 & 00:00 & No Race
& 15 & 00:00 & No Race 
& 597 & 00:00 & Race* \\ \hline

ForAll With & 38 & 2
%& 9 & 00:00 & No Race
& 9 & 00:00 & No Race
& 9 & 00:00 & No Race 
& N/A & N/A & N/A \\ 
Iterable &  & 
& & &
& & &
& & & \\ \hline

Integer Counter  & 54 & 10
%& 24 & 00:01 & No Race
& 24 & 00:01 & No Race
& 1013102 & 05:53 & No Race 
& N/A & N/A & N/A \\ 
Isolated &  & 
& & &
& & &
& & & \\ \hline


Pipeline With & 69 & 5
%& 34 & 0:00:00 & No Race
& 34 & 00:00 & No Race
& 34 & 00:00 & No Race 
& N/A & N/A & N/A \\ 
Futures &  & 
& & &
& & &
& & & \\ \hline

Substring Search  & 83 & 59 
%& 64 & 00:03 & Race
& 64 & 00:03 & Race
& 8 & 00:00 & Race 
& N/A & N/A & N/A \\ \hline

Binary Trees & 80 & 525 
%& 632 & 0:00:05 & No Race
& 630 & 00:25 & No Race
& 632 & 00:03 & No Race 
& N/A & N/A & N/A \\ \hline

Prime Num & 51 & 25
%& 776 & 00:01 & No Race
& 776 & 00:01 & No Race
& 3,542,569 & 17:37 & No Race 
& N/A & N/A & N/A \\ 
Counter &  & 
& & &
& & &
& & & \\ \hline

Prime Num  & 52 & 25
%& 30 & 0:00:02 & Race*
& 30 & 00:02 & No Race
& 18 & 00:01 & No Race
& N/A & N/A & N/A \\
Counter ForAll &  & 
& & &
& & &
& & & \\ \hline

Prime Num & 44 & 11 
%& 653 & 0:00:01 & No Race
& 653 & 00:01 & No Race
& 2,528,064 & 15:44 & No Race 
& N/A & N/A & N/A \\
Counter ForAsync &  & 
& & &
& & &
& & & \\ \hline
Reciprocal Array & 58 & 2
%& 12 & 0:00:16 & Race
& 4 & 00:08 & Race
& 32 & 00:06 & Race
& N/A & N/A & N/A \\ 
Sum &  & 
& & &
& & &
& & & \\ \hline

Add  & 67 & 3 
%& 11 & 0:00:01 & No Race 
& 11 & 00:01 & No Race 
& 62,374 & 00:33 & No Race
& 4930 & 00:03 & Race* \\ \hline

Scalar Multiply  & 55 & 3 
%& 15 & 0:00:01 & No Race
& 15 & 00:01 & No Race
& 55,712 & 00:30 & No Race 
& 826 & 00:01 & Race* \\ \hline

Vector Add  & 50 & 3 
%& 5 & 0:00:01 & No Race
& 5 & 00:00 & No Race
& 17 & 00:00 & No Race 
& 46,394 & 00:19 & No Race \\ \hline

Clumped Access  & 30 & 3 
%& 9 & 0:00:07 & No Race
& 5 & 00:03 & No Race
& 15 & 00:00 & No Race 
& N/A & N/A & N/A \\ \hline

\end{tabular}
\end{table*}
\end{landscape}

Table \ref{tab:results} presents the results of verification of HJ benchmarks using computation graphs based data race detector described in this work, permission regions based extension of JPF and precise race detector extension of JPF. The number of states explored by JPF and time required for verification by each of these methods were compared. The tests were run for a maximum of an hour before they were terminated manually. For the tests that did not finish execution within the stipulated time or the ones that ran out of JVM heap memory were considered failed and marked as N/A in the table. The error note column shows the results of verification. The tests that produced erroneous results were marked with an asterisk ($\ast$). 

The benchmarks used in this study make use of various constructs of HJ for achieving task parallelism. They spawn a wide range of tasks with smaller programs having 3-15 tasks going all the way upto 525 tasks for larger tasks. The experiments were run on a machine with an Intel Core i5 processor with 2.6GHz speed and 8GB of RAM. The number of cores used by the system is not relevant since since JPF runs only a single thread at a time.

The Precise race detector inserts choices in the scheduler for all thread actions such as thread creation, synchronizations, locks etc. Therefore, it does not complete execution within the stipulated time or runs out of memory even on smaller programs because of the state space explosion. It also reports race for Two Dimensional Arrays, Scalar multiply and Vector Add benchmarks where no data race actually exists in the program. This is because in precise race detector, the access on an array object looks like a data race since it is not able to see the difference in the indexes.

The Gradual Permission regions based detector works pretty well compared to precise race detector. The number of states explored and time required for data race analysis by Permission regions and computation graphs is almost the same when the tasks don't access shared variables outside isolated blocks. When there are accesses to shared variables, the state space of permission regions grows very fast since the shared variable accesses have to annotated with regions and race detector creates scheduling choices at every region boundary. Analyzing a single computation graph for a program is enough for a program without isolated blocks since by Lemma \ref{lem:drf}, if a computation graph of a program is data race free, all computation graphs from all schedules are data race free. Therefore, computation graphs are built using a single program schedule. This difference can be seen in examples for Add, Scalar multiply and Prime number counter benchmarks. These benchmarks use shared variables that have to be enclosed within regions which results in a large state space for permission regions and longer analysis time.

\begin{table}
\centering
\caption{Comparison of results for on-the-fly and normal data race detection using computation graphs.}
\label{tab:otf}
\begin{tabular}{|c|c|c|c|c|}
\hline
    	{Num of Tasks (n)} & 
		 \multicolumn{2}{c|}{On-the-fly} &
		\multicolumn{2}{c|}{Normal} \\ \hline
		
 & States & Time & States & Time  \\ \hline
5  & 5 & 00:01  & 14 & 00:01  \\ \hline
50  & 5 & 00:01  & 61 & 00:03  \\ \hline
100  & 5 & 00:01  & 112 & 00:05  \\ \hline
500  & 5 & 00:01  & 513 & 02:25  \\ \hline
1000  & 5 & 00:01  & 1013 & 08:34  \\ \hline

\end{tabular}
\end{table}

\begin{figure}
  \begin{center}
    \begin{lstlisting}
public class Example{
	static int x = 0;
	public static void main(String[] args) {
		finish {
			async { //Task1
				x = x + 1;
			}
			async { //Task2
				x = x + 2;
			}
		}
		forall (1, n, (index)){
			x = x + index;
		}
	}
}
\end{lstlisting}
  \end{center}
  \caption{An HJ Program for comparing on-the-fly with normal data race detection using computation graphs.}
  \label{fig:hj-otf}
\end{figure}

Table \ref{tab:otf} presents the improvement offered by on-the-fly analysis over normal data race detection  using computation graphs. The example in \figref{fig:hj-otf} is used to demonstrate the difference in performance of these techniques under different program sizes. 

The example in \figref{fig:hj-otf} is implemented in HJ. It consists of a \textbf{finish} block with two \textbf{async} tasks that have a data race on static variable x. This \textbf{finish} block is followed by a \textbf{forall} loop which is used to control the size of the program. Note that a \textbf{forall} loop has an implicit \textbf{finish} block and each of the iterations of the loop are executed by creating \textbf{async} tasks inside the \textbf{finish} block. 

From the results in table \ref{tab:otf}, it can be observed that the time required to analyze the program using normal data race detection using computation graphs increases as the size of the program increases. For the on-the-fly data race detection, the race detection is run on a \textbf{finish} block as soon as the \textbf{finish} block completes execution. Since, a data race is present in the first \textbf{finish} block itself, the entire program is not executed. Therefore, the time required for analysis is very low.
